{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8087/3623195152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/Cora_mask1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/Cora_mask2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('logs/Cora_mask1.csv')\n",
    "data2 = pd.read_csv('logs/Cora_mask2.csv')\n",
    "# 去除最后一行\n",
    "data = data[:-1]\n",
    "data2 = data2[:-1]\n",
    "\n",
    "# 在同一张图上画多个曲线\n",
    "# 设置图的大小和清晰度\n",
    "plt.figure(dpi=600,figsize=(20, 5))\n",
    "plt.plot(data['epoch'], data['loss'], label='mask1')\n",
    "plt.plot(data2['epoch'], data2['loss'], label='mask2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/graph/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'coo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m adj \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39madjacency_matrix()\u001b[38;5;241m.\u001b[39mcoalesce()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 使用索引操作获取所有与输入节点相连的节点\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m rows, cols, _ \u001b[38;5;241m=\u001b[39m \u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo\u001b[49m()\n\u001b[1;32m     18\u001b[0m neighs \u001b[38;5;241m=\u001b[39m cols[rows \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_nodes)\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 打印结果\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'coo'"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 构建有向图\n",
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(5)\n",
    "g.add_edges([0, 1, 1, 2, 3, 4], [1, 2, 3, 0, 4, 0]) # 添加边\n",
    "\n",
    "# 定义输入节点列表\n",
    "input_nodes = [0, 3]\n",
    "\n",
    "# 将邻接矩阵转换为 COO 格式，并移动到 GPU 上\n",
    "device = torch.device('cuda')\n",
    "adj = g.adjacency_matrix().coalesce().to(device)\n",
    "\n",
    "# 使用索引操作获取所有与输入节点相连的节点\n",
    "rows, cols, _ = adj.coo()\n",
    "neighs = cols[rows == torch.tensor(input_nodes).to(device)]\n",
    "\n",
    "# 打印结果\n",
    "print(neighs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCUDA.cpp:26496 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m adj_matrix \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39madjacency_matrix()\n\u001b[1;32m     40\u001b[0m \u001b[39m# 获得这些节点的邻居\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m neighbors \u001b[39m=\u001b[39m adj_matrix[nodes]\u001b[39m.\u001b[39mto_dense()\n\u001b[1;32m     43\u001b[0m \u001b[39m# 将邻居节点合并为一个无重复的点集\u001b[39;00m\n\u001b[1;32m     44\u001b[0m unique_neighbors, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(neighbors, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCUDA.cpp:26496 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# import dgl\n",
    "\n",
    "# # 创建一个包含浮点数节点索引的张量\n",
    "# node_indices = torch.tensor([0.0, 1.0, 2.0])\n",
    "\n",
    "# # 将节点索引张量转换为整数类型\n",
    "# node_indices = node_indices.long()\n",
    "\n",
    "# # 创建一个图对象\n",
    "# num_nodes = 5\n",
    "# edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2),  (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "# graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "\n",
    "\n",
    "# # 将图对象转换为邻接矩阵\n",
    "# adj_matrix =graph.adjacency_matrix()\n",
    "\n",
    "# # 选择节点3\n",
    "# node = torch.tensor([3])\n",
    "\n",
    "# # 获得节点3的邻居\n",
    "# neighbors = adj_matrix[node]\n",
    "\n",
    "# print(neighbors)\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 创建一个图对象\n",
    "num_nodes = 5\n",
    "edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2), (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "\n",
    "# 选择多个节点\n",
    "nodes = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 将图对象转换为邻接矩阵\n",
    "adj_matrix = graph.adjacency_matrix()\n",
    "\n",
    "# 获得这些节点的邻居\n",
    "neighbors = adj_matrix[nodes].to_dense()\n",
    "\n",
    "# 将邻居节点合并为一个无重复的点集\n",
    "unique_neighbors, _ = torch.unique(neighbors, dim=1, return_inverse=True)\n",
    "\n",
    "print(unique_neighbors.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor(indices=tensor([[0, 0],\n",
      "                       [1, 3]]),\n",
      "       values=tensor([1., 1.]),\n",
      "       size=(1, 5), nnz=2, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 创建一个图对象\n",
    "num_nodes = 5\n",
    "edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2), (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "graph.to(torch.device('cuda'))\n",
    "\n",
    "# 选择多个节点\n",
    "nodes = torch.tensor([0])\n",
    "\n",
    "def get_neighbors(graph, nodes):\n",
    "\n",
    "\n",
    "    assert type(nodes) == torch.Tensor, 'nodes must be a tensor'\n",
    "    # 将图对象转换为邻接矩阵\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "\n",
    "    print(nodes)\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "\n",
    "\n",
    "    print(neighbors)\n",
    "\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "\n",
    "    neighbors.indices()[1]\n",
    "\n",
    "    # unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph, nodes, depth):\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n",
    "\n",
    "# 获得节点0的所有邻居\n",
    "all_neighbors = get_all_neighbors(graph, nodes, 1)\n",
    "all_neighbors\n",
    "\n",
    "# # 将邻居节点合并为一个无重复的点集\n",
    "# unique_neighbors, _ = torch.unique(neighbors, dim=1, return_inverse=True)\n",
    "\n",
    "# print(unique_neighbors.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor):\n",
    "    \"\"\"获得一个中心点列表中所有中心点的邻居，不包括中心点本身，且去重\n",
    "\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 图对象\n",
    "        nodes (torch.Tensor): 中心节点张量，1*nodes_num\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    assert nodes.is_cuda == True\n",
    "    # print(type(graph))\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "    # 判断adj_matrix的是否在GPU上\n",
    "    # print(adj_matrix.is_cuda)\n",
    "    adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # print(adj_matrix.device)\n",
    "    assert adj_matrix.is_cuda == True\n",
    "\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "    # neighbors.indices()返回一个2*neighbors_num的矩阵，2 * neighbors_num，第二行是所有邻居节点标号的横向拼接\n",
    "    unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor, depth:int):\n",
    "    \"\"\"多重递归获得邻居节点集合张量（逻辑上是集合，物理数据结构为torch.Tensor），且去重\n",
    "\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 输入图\n",
    "        nodes (torch.Tensor): 中心节点集合张量，1*nodes_num\n",
    "        depth (int): 递归深度，为0时相当于点遮盖，直接返回nodes\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        neighbors = torch.cat([nodes, neighbors])\n",
    "        neighbors = torch.unique(neighbors)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(base) \u001b]0;root@84dc942e2771: ~/models/SMCL\u0007\u001b[01;32mroot@84dc942e2771\u001b[00m:\u001b[01;34m~/models/SMCL\u001b[00m# ^C\n",
      "\n",
      "(base) \u001b]0;root@84dc942e2771: ~/models/SMCL\u0007\u001b[01;32mroot@84dc942e2771\u001b[00m:\u001b[01;34m~/models/SMCL\u001b[00m# "
     ]
    }
   ],
   "source": [
    "!bash ;conda activate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def get_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor):\n",
    "    \"\"\"获得一个中心点列表中所有中心点的邻居，不包括中心点本身，且去重\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 图对象\n",
    "        nodes (torch.Tensor): 中心节点张量，1*nodes_num\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    assert nodes.is_cuda == True\n",
    "    # print(type(graph))\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "    # 判断adj_matrix的是否在GPU上\n",
    "    # print(adj_matrix.is_cuda)\n",
    "    adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # print(adj_matrix.device)\n",
    "    assert adj_matrix.is_cuda == True\n",
    "\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "    # neighbors.indices()返回一个2*neighbors_num的矩阵，2 * neighbors_num，第二行是所有邻居节点标号的横向拼接\n",
    "    unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor, depth:int):\n",
    "    \"\"\"多重递归获得邻居节点集合张量（逻辑上是集合，物理数据结构为torch.Tensor），且去重\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 输入图\n",
    "        nodes (torch.Tensor): 中心节点集合张量，1*nodes_num\n",
    "        depth (int): 递归深度，为0时相当于点遮盖，直接返回nodes\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 9], device='cuda:0')\n",
      "tensor([2, 3, 4, 7, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "# import numpy as np\n",
    "\n",
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(10)\n",
    "g.add_edges([0, 1, 1, 2, 2, 3, 3, 4, 5, 6, 7, 8, 9, 0, 5, 6, 7, 8, 9, 4],\n",
    "            [1, 2, 3, 0, 4, 5, 6, 7, 8, 9, 0, 1, 2, 6, 7, 8, 9, 3, 4, 5])\n",
    "g = g.to(torch.device('cuda'))\n",
    "\n",
    "nodes = torch.tensor([1,5,9],device=\"cuda\")\n",
    "\n",
    "print(nodes)\n",
    "print(get_all_neighbors(g,nodes,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_mean:  0.5002698412698413\n",
      "acc_max 0.522\n",
      "acc_minus:  0.025603174603174628\n",
      "acc_plus:  0.02173015873015871\n",
      "[0.48933333333333334, 0.4933333333333333, 0.504, 0.5113333333333333, 0.5066666666666666, 0.5, 0.5113333333333333, 0.4913333333333334, 0.5046666666666667, 0.4913333333333333, 0.5133333333333334, 0.484, 0.4953333333333334, 0.5166666666666667, 0.5133333333333333, 0.5033333333333334, 0.5046666666666667, 0.5066666666666668, 0.4986666666666667, 0.4746666666666667, 0.4946666666666667, 0.5013333333333334, 0.5113333333333332, 0.5026666666666666, 0.4953333333333333, 0.49266666666666675, 0.5093333333333334, 0.5079999999999999, 0.4946666666666667, 0.502, 0.4846666666666667, 0.5033333333333333, 0.4873333333333334, 0.49066666666666664, 0.4926666666666667, 0.4993333333333333, 0.502, 0.4966666666666667, 0.522, 0.49800000000000005, 0.4993333333333333, 0.5093333333333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 读取文件中的字符串内容\n",
    "data = open(\"./nohup_logs/20230515002301/IMDB-BINARY.log\").read()\n",
    "# 按照换行符和逗号分割字符串\n",
    "data_ls = data.split(\"\\n\")\n",
    "acc_ls = []\n",
    "for i in data_ls:\n",
    "    for j in i.split(\",\"):\n",
    "        if (j.startswith(\"Acc\")):\n",
    "            # print(j)\n",
    "            j.replace(\"Acc: \",\"\")\n",
    "            acc = float(j.split(\": \")[1])\n",
    "            acc_ls.append(acc)\n",
    "# 计算acc的平均值和+/-误差\n",
    "acc_mean = np.mean(acc_ls)\n",
    "print(\"acc_mean: \",acc_mean)\n",
    "print(\"acc_max\", np.max(acc_ls))\n",
    "# 计算负误差\n",
    "acc_minus = acc_mean - np.min(acc_ls)\n",
    "acc_plus = np.max(acc_ls) - acc_mean\n",
    "print(\"acc_minus: \",acc_minus)\n",
    "print(\"acc_plus: \",acc_plus)\n",
    "print(acc_ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
