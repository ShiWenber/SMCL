{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8087/3623195152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/Cora_mask1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/Cora_mask2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('logs/Cora_mask1.csv')\n",
    "data2 = pd.read_csv('logs/Cora_mask2.csv')\n",
    "# 去除最后一行\n",
    "data = data[:-1]\n",
    "data2 = data2[:-1]\n",
    "\n",
    "# 在同一张图上画多个曲线\n",
    "# 设置图的大小和清晰度\n",
    "plt.figure(dpi=600,figsize=(20, 5))\n",
    "plt.plot(data['epoch'], data['loss'], label='mask1')\n",
    "plt.plot(data2['epoch'], data2['loss'], label='mask2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/graph/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'coo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m adj \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39madjacency_matrix()\u001b[38;5;241m.\u001b[39mcoalesce()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 使用索引操作获取所有与输入节点相连的节点\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m rows, cols, _ \u001b[38;5;241m=\u001b[39m \u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo\u001b[49m()\n\u001b[1;32m     18\u001b[0m neighs \u001b[38;5;241m=\u001b[39m cols[rows \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_nodes)\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 打印结果\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'coo'"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 构建有向图\n",
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(5)\n",
    "g.add_edges([0, 1, 1, 2, 3, 4], [1, 2, 3, 0, 4, 0]) # 添加边\n",
    "\n",
    "# 定义输入节点列表\n",
    "input_nodes = [0, 3]\n",
    "\n",
    "# 将邻接矩阵转换为 COO 格式，并移动到 GPU 上\n",
    "device = torch.device('cuda')\n",
    "adj = g.adjacency_matrix().coalesce().to(device)\n",
    "\n",
    "# 使用索引操作获取所有与输入节点相连的节点\n",
    "rows, cols, _ = adj.coo()\n",
    "neighs = cols[rows == torch.tensor(input_nodes).to(device)]\n",
    "\n",
    "# 打印结果\n",
    "print(neighs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCUDA.cpp:26496 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m adj_matrix \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39madjacency_matrix()\n\u001b[1;32m     40\u001b[0m \u001b[39m# 获得这些节点的邻居\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m neighbors \u001b[39m=\u001b[39m adj_matrix[nodes]\u001b[39m.\u001b[39mto_dense()\n\u001b[1;32m     43\u001b[0m \u001b[39m# 将邻居节点合并为一个无重复的点集\u001b[39;00m\n\u001b[1;32m     44\u001b[0m unique_neighbors, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(neighbors, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index.Tensor' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index.Tensor' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterCUDA.cpp:26496 [kernel]\nQuantizedCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# import dgl\n",
    "\n",
    "# # 创建一个包含浮点数节点索引的张量\n",
    "# node_indices = torch.tensor([0.0, 1.0, 2.0])\n",
    "\n",
    "# # 将节点索引张量转换为整数类型\n",
    "# node_indices = node_indices.long()\n",
    "\n",
    "# # 创建一个图对象\n",
    "# num_nodes = 5\n",
    "# edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2),  (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "# graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "\n",
    "\n",
    "# # 将图对象转换为邻接矩阵\n",
    "# adj_matrix =graph.adjacency_matrix()\n",
    "\n",
    "# # 选择节点3\n",
    "# node = torch.tensor([3])\n",
    "\n",
    "# # 获得节点3的邻居\n",
    "# neighbors = adj_matrix[node]\n",
    "\n",
    "# print(neighbors)\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 创建一个图对象\n",
    "num_nodes = 5\n",
    "edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2), (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "\n",
    "# 选择多个节点\n",
    "nodes = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 将图对象转换为邻接矩阵\n",
    "adj_matrix = graph.adjacency_matrix()\n",
    "\n",
    "# 获得这些节点的邻居\n",
    "neighbors = adj_matrix[nodes].to_dense()\n",
    "\n",
    "# 将邻居节点合并为一个无重复的点集\n",
    "unique_neighbors, _ = torch.unique(neighbors, dim=1, return_inverse=True)\n",
    "\n",
    "print(unique_neighbors.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor(indices=tensor([[0, 0],\n",
      "                       [1, 3]]),\n",
      "       values=tensor([1., 1.]),\n",
      "       size=(1, 5), nnz=2, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "# 创建一个图对象\n",
    "num_nodes = 5\n",
    "edges = [(0,1),(1,2),(1,3),(2,4),(3,0),(3,2), (1,0),(2,1),(3,1),(4,2),(0,3),(2,3)] \n",
    "graph = dgl.graph(edges, num_nodes=num_nodes)\n",
    "graph.to(torch.device('cuda'))\n",
    "\n",
    "# 选择多个节点\n",
    "nodes = torch.tensor([0])\n",
    "\n",
    "def get_neighbors(graph, nodes):\n",
    "\n",
    "\n",
    "    assert type(nodes) == torch.Tensor, 'nodes must be a tensor'\n",
    "    # 将图对象转换为邻接矩阵\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "\n",
    "    print(nodes)\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "\n",
    "\n",
    "    print(neighbors)\n",
    "\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "\n",
    "    neighbors.indices()[1]\n",
    "\n",
    "    # unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph, nodes, depth):\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n",
    "\n",
    "# 获得节点0的所有邻居\n",
    "all_neighbors = get_all_neighbors(graph, nodes, 1)\n",
    "all_neighbors\n",
    "\n",
    "# # 将邻居节点合并为一个无重复的点集\n",
    "# unique_neighbors, _ = torch.unique(neighbors, dim=1, return_inverse=True)\n",
    "\n",
    "# print(unique_neighbors.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor):\n",
    "    \"\"\"获得一个中心点列表中所有中心点的邻居，不包括中心点本身，且去重\n",
    "\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 图对象\n",
    "        nodes (torch.Tensor): 中心节点张量，1*nodes_num\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    assert nodes.is_cuda == True\n",
    "    # print(type(graph))\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "    # 判断adj_matrix的是否在GPU上\n",
    "    # print(adj_matrix.is_cuda)\n",
    "    adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # print(adj_matrix.device)\n",
    "    assert adj_matrix.is_cuda == True\n",
    "\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "    # neighbors.indices()返回一个2*neighbors_num的矩阵，2 * neighbors_num，第二行是所有邻居节点标号的横向拼接\n",
    "    unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor, depth:int):\n",
    "    \"\"\"多重递归获得邻居节点集合张量（逻辑上是集合，物理数据结构为torch.Tensor），且去重\n",
    "\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 输入图\n",
    "        nodes (torch.Tensor): 中心节点集合张量，1*nodes_num\n",
    "        depth (int): 递归深度，为0时相当于点遮盖，直接返回nodes\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        neighbors = torch.cat([nodes, neighbors])\n",
    "        neighbors = torch.unique(neighbors)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(base) \u001b]0;root@84dc942e2771: ~/models/SMCL\u0007\u001b[01;32mroot@84dc942e2771\u001b[00m:\u001b[01;34m~/models/SMCL\u001b[00m# ^C\n",
      "\n",
      "(base) \u001b]0;root@84dc942e2771: ~/models/SMCL\u0007\u001b[01;32mroot@84dc942e2771\u001b[00m:\u001b[01;34m~/models/SMCL\u001b[00m# "
     ]
    }
   ],
   "source": [
    "!bash ;conda activate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "def get_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor):\n",
    "    \"\"\"获得一个中心点列表中所有中心点的邻居，不包括中心点本身，且去重\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 图对象\n",
    "        nodes (torch.Tensor): 中心节点张量，1*nodes_num\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    assert nodes.is_cuda == True\n",
    "    # print(type(graph))\n",
    "    adj_matrix = graph.adjacency_matrix()\n",
    "    # 判断adj_matrix的是否在GPU上\n",
    "    # print(adj_matrix.is_cuda)\n",
    "    adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # adj_matrix = adj_matrix.to(nodes.device)\n",
    "    # print(adj_matrix.device)\n",
    "    assert adj_matrix.is_cuda == True\n",
    "\n",
    "    # 获得这些节点的邻居\n",
    "    neighbors = adj_matrix.index_select(0, nodes)\n",
    "    # 压缩稀疏矩阵\n",
    "    neighbors = neighbors.coalesce()\n",
    "    # neighbors.indices()返回一个2*neighbors_num的矩阵，2 * neighbors_num，第二行是所有邻居节点标号的横向拼接\n",
    "    unique_neighbors = torch.unique(neighbors.indices()[1])\n",
    "\n",
    "    return unique_neighbors\n",
    "\n",
    "# 多重递归获得邻居节点\n",
    "def get_all_neighbors(graph:dgl.DGLGraph, nodes:torch.Tensor, depth:int):\n",
    "    \"\"\"多重递归获得邻居节点集合张量（逻辑上是集合，物理数据结构为torch.Tensor），且去重\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): 输入图\n",
    "        nodes (torch.Tensor): 中心节点集合张量，1*nodes_num\n",
    "        depth (int): 递归深度，为0时相当于点遮盖，直接返回nodes\n",
    "    Returns:\n",
    "        torch.Tensor: 所有中心点的邻居节点组成的无重复节点张量, 1*neighbors_num\n",
    "    \"\"\"\n",
    "    if depth == 0:\n",
    "        return nodes\n",
    "    else:\n",
    "        neighbors = get_neighbors(graph, nodes)\n",
    "        return get_all_neighbors(graph, neighbors, depth-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 9], device='cuda:0')\n",
      "tensor([2, 3, 4, 7, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "# import numpy as np\n",
    "\n",
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(10)\n",
    "g.add_edges([0, 1, 1, 2, 2, 3, 3, 4, 5, 6, 7, 8, 9, 0, 5, 6, 7, 8, 9, 4],\n",
    "            [1, 2, 3, 0, 4, 5, 6, 7, 8, 9, 0, 1, 2, 6, 7, 8, 9, 3, 4, 5])\n",
    "g = g.to(torch.device('cuda'))\n",
    "\n",
    "nodes = torch.tensor([1,5,9],device=\"cuda\")\n",
    "\n",
    "print(nodes)\n",
    "print(get_all_neighbors(g,nodes,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_mean:  0.7442438532282282\n",
      "acc_max 0.7529440154440155\n",
      "acc_minus:  0.003019593253968167\n",
      "acc_plus:  0.008700162215787288\n",
      "[0.7529440154440155, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426, 0.7421332046332045, 0.7439269626769626, 0.7529440154440155, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426, 0.7520431145431146, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426, 0.7421332046332045, 0.7439269626769626, 0.7529440154440155, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426, 0.7529440154440155, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426, 0.7421332046332045, 0.7439269626769626, 0.7520431145431146, 0.7439752252252252, 0.7439510939510939, 0.7457368082368083, 0.7412483912483913, 0.7421412483912485, 0.74122425997426]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 读取文件中的字符串内容\n",
    "data = open(\"./nohup_logs/20230517131004/PROTEINS.log\").read()\n",
    "# 按照换行符和逗号分割字符串\n",
    "data_ls = data.split(\"\\n\")\n",
    "acc_ls = []\n",
    "for i in data_ls:\n",
    "    for j in i.split(\",\"):\n",
    "        if (j.startswith(\"Acc\")):\n",
    "            # print(j)\n",
    "            j.replace(\"Acc: \",\"\")\n",
    "            acc = float(j.split(\": \")[1])\n",
    "            acc_ls.append(acc)\n",
    "# 计算acc的平均值和+/-误差\n",
    "acc_mean = np.mean(acc_ls)\n",
    "print(\"acc_mean: \",acc_mean)\n",
    "print(\"acc_max\", np.max(acc_ls))\n",
    "# 计算负误差\n",
    "acc_minus = acc_mean - np.min(acc_ls)\n",
    "acc_plus = np.max(acc_ls) - acc_mean\n",
    "print(\"acc_minus: \",acc_minus)\n",
    "print(\"acc_plus: \",acc_plus)\n",
    "print(acc_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=218, num_edges=960,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "\n",
    "# 加载IMDB-BINARY数据集\n",
    "dataset = TUDataset(\"REDDIT-BINARY\")\n",
    "\n",
    "# 获取数据集中的第一个图\n",
    "graph, label = dataset[0]\n",
    "\n",
    "# 将图数据转换为DGLGraph对象\n",
    "g = dgl.to_homogeneous(graph)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdgl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m TUDataset\n\u001b[1;32m      6\u001b[0m \u001b[39m# 将图数据转换为DGLGraph对象\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m g \u001b[39m=\u001b[39m dgl\u001b[39m.\u001b[39mgraph(graph)\n\u001b[1;32m      9\u001b[0m \u001b[39m# 可视化图\u001b[39;00m\n\u001b[1;32m     10\u001b[0m nx_g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mto_networkx(node_attrs\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from dgl.data import TUDataset\n",
    "\n",
    "# 将图数据转换为DGLGraph对象\n",
    "g = dgl.graph(graph)\n",
    "\n",
    "# 可视化图\n",
    "nx_g = g.to_networkx(node_attrs=['feat'])\n",
    "pos = nx.spring_layout(nx_g)\n",
    "draw(g, pos, node_size=20, node_color='b', with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(g\u001b[39m.\u001b[39mnum_nodes())\n\u001b[1;32m     14\u001b[0m \u001b[39m# 2. 随机选择一些中心点\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m central_nodes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor([\u001b[39m3\u001b[39;49m,\u001b[39m5\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\n\u001b[1;32m     17\u001b[0m \u001b[39m# 3. 构造一个onehot矩阵\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_hot\u001b[39m(y, num_class):\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "\n",
    "# 1. 构造一个图\n",
    "g = dgl.graph(([0, 1, 3, 6, 3, 5],\n",
    "               [1, 2, 4, 7, 1, 1]))\n",
    "g = dgl.to_bidirected(g)\n",
    "## 输出节点数量\n",
    "print(g.num_nodes())\n",
    "\n",
    "# 2. 随机选择一些中心点\n",
    "central_nodes = torch.Tensor([3,5], dtype=torch.long)\n",
    "\n",
    "# 3. 构造一个onehot矩阵\n",
    "def one_hot(y, num_class):\n",
    "    return torch.eye(num_class)[y]\n",
    "onehot_cn = one_hot(central_nodes, g.num_nodes()) # 2 * 8\n",
    "print(onehot_cn)\n",
    "\n",
    "# 4. 乘邻接矩阵 (乘k次表示取出k层邻居)\n",
    "onehot_cn * adj  # 2 * 8 x 8 * 8 = 2 * 8\n",
    "\n",
    "# 5. 邻接矩阵中大于1的值表示子图中包含该点，大于1的部分变为1，每行代表一个子图\n",
    "\n",
    "# 将所有的子图节点全部合为集合 m 个子图，1 * m x 5中的结果，大于1的部分变为1\n",
    "\n",
    "# 6. 训练，所有子图一起输入gcn, 用张量积取得特征\n",
    "\n",
    "# 7. 训练完后，矩阵为 8 * 256\n",
    "\n",
    "# 8. 2 * 8 x 8*256\n",
    "\n",
    "\n",
    "\n",
    "# 将DGL图转换为networkx图\n",
    "nx_g = g.to_networkx()\n",
    "\n",
    "# 创建matplotlib图形对象\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# 可视化networkx图\n",
    "pos = nx.kamada_kawai_layout(nx_g)\n",
    "nx.draw(nx_g, pos=pos, with_labels=True, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# 输出g的邻接矩阵\n",
    "print(g.adjacency_matrix())\n",
    "\n",
    "# 生成邻接矩阵\n",
    "adj_matrix = g.adjacency_matrix()\n",
    "\n",
    "# 将稀疏矩阵转换为标准矩阵\n",
    "dense_matrix = adj_matrix.toarray()\n",
    "\n",
    "# 打印标准矩阵\n",
    "print(dense_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
